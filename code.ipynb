{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16cd63cf",
   "metadata": {
    "id": "16cd63cf"
   },
   "source": [
    "# CSCI 5521 — HW1 (CODE)\n",
    "\n",
    "**This notebook contains only the coding parts and you will need to submit a separate PDF for other parts of the homework.** It mirrors the MATLAB file structure in Python. But, instead of having separate files, we will implement the following in separate cells of this notebook:\n",
    "- `MLE_Learning` (learn Bernoulli parameters per class)\n",
    "- `Bayes_Testing` (classify a dataset, compute error rate)\n",
    "- `Bayes_Learning` (sweep priors on validation, pick the best)\n",
    "- (Given) A driver cell that behaves like `HW1_script.m`, its MATLAB counterpart (Parts 1–3).\n",
    "\n",
    "**INSTRUCTIONS**\n",
    "- Make a copy of this template file to modify the file.\n",
    "- Download and upload the 4 data files on to the Colab environment. Do not modify the provided data files. Provided files:\n",
    "  - toy_data.txt\n",
    "  - training_data.txt\n",
    "  - validation_data.txt\n",
    "  - testing_dtaa.txt\n",
    "- Implement the `TODO` sections\n",
    "\n",
    "**SUBMISSION**\n",
    "\n",
    "If you choose to do your homework in Python, you can complete and submit this file alongside a PDF file with the rest of your answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1583030f",
   "metadata": {
    "id": "1583030f"
   },
   "source": [
    "## Setup (Imports, Data Paths and Data Loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbe99f93",
   "metadata": {
    "id": "dbe99f93"
   },
   "outputs": [],
   "source": [
    "# TODO: Edit if your files are elsewhere\n",
    "TOY_PATH   = \"dataset/toy_data.txt\"\n",
    "TRAIN_PATH = \"dataset/training_data.txt\"\n",
    "VALID_PATH = \"dataset/validation_data.txt\"\n",
    "TEST_PATH  = \"dataset/testing_data.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce94ad19",
   "metadata": {
    "id": "ce94ad19"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def load_txt_dataset(path: str):\n",
    "    arr = np.loadtxt(path)\n",
    "    if arr.ndim == 1:\n",
    "        arr = arr[None, :]\n",
    "    X = arr[:, :-1].astype(int)\n",
    "    y = arr[:, -1].astype(int)\n",
    "    # Minimal checks\n",
    "    if not set(np.unique(y)).issubset({1, 2}):\n",
    "        raise ValueError(f\"Labels must be 1 or 2: found {sorted(set(np.unique(y)))} in {path}\")\n",
    "    uniq = set(np.unique(X)) if X.size else set()\n",
    "    if uniq - {0, 1}:\n",
    "        raise ValueError(f\"Features must be 0/1 only: found {sorted(uniq)} in {path}\")\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6436dfe",
   "metadata": {
    "id": "d6436dfe"
   },
   "source": [
    "## Part A — `MLE_Learning(X, y)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "668ce61d",
   "metadata": {
    "id": "668ce61d"
   },
   "outputs": [],
   "source": [
    "def MLE_Learning(training_data):\n",
    "    \"\"\"\n",
    "    Return:\n",
    "      p1: Bernoulli parameters for class 1 (P(x_j=0 | C1))\n",
    "      p2: Bernoulli parameters for class 2 (P(x_j=0 | C2))\n",
    "      pc1: prior for class 1\n",
    "      pc2: prior for class 2\n",
    "    \"\"\"\n",
    "    # - Separate features (X) from labels\n",
    "    data = np.array(training_data)\n",
    "    X = data[:, :-1]  \n",
    "    y = data[:, -1]   \n",
    "    N = X.shape[0]  \n",
    "    D = X.shape[1]  \n",
    "    \n",
    "    X_C1 = X[y == 1]\n",
    "    X_C2 = X[y == 2]\n",
    "\n",
    "    # - Count samples per class\n",
    "    N1 = len(X_C1) \n",
    "    N2 = len(X_C2) \n",
    "\n",
    "    # - Compute pc1 and pc2 (Class Priors)\n",
    "    pc1 = N1 / N\n",
    "    pc2 = N2 / N\n",
    "\n",
    "    # - Compute p1 and p2 (Bernoulli Parameters P(x_j=0 | C_i))\n",
    "    if N1 > 0:\n",
    "        sum_x1_C1 = np.sum(X_C1, axis=0) \n",
    "        sum_x0_C1 = N1 - sum_x1_C1\n",
    "        p1 = sum_x0_C1 / N1\n",
    "    else:\n",
    "        p1 = np.full(D, 0.5) # The Laplace Smoothed estimate is 0.5. Use this to fill in if there is no sample in C1.\n",
    "\n",
    "    if N2 > 0:\n",
    "        sum_x1_C2 = np.sum(X_C2, axis=0) \n",
    "        sum_x0_C2 = N2 - sum_x1_C2\n",
    "        p2 = sum_x0_C2 / N2\n",
    "    else:\n",
    "        p2 = np.full(D, 0.5)\n",
    "\n",
    "    return p1, p2, pc1, pc2\n",
    "\n",
    "    # TODO: Implement MLE estimation as in MATLAB version\n",
    "    # - Separate features (X) from labels\n",
    "    # - Count samples per class\n",
    "    # - Compute p1 and p2\n",
    "    # - Compute pc1 and pc2\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25353400",
   "metadata": {
    "id": "25353400"
   },
   "source": [
    "## Part B — `Bayes_Testing(data, p1, p2, pc1, pc2)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9bed70a",
   "metadata": {
    "id": "d9bed70a"
   },
   "outputs": [],
   "source": [
    "def Bayes_Testing(data, p1, p2, pc1, pc2):\n",
    "    \"\"\"\n",
    "    Return:\n",
    "      test_error: fraction of misclassified samples using given parameters and priors\n",
    "    \"\"\"\n",
    "    X = data[:, :-1]  \n",
    "    y = data[:, -1]   \n",
    "    N = X.shape[0]  \n",
    "\n",
    "    errors = 0\n",
    "    small_prob = 1e-10  # To avoid log(0)\n",
    "\n",
    "    log_p1_0 = np.log(p1 + small_prob)\n",
    "    log_p1_1 = np.log(1 - p1 + small_prob)\n",
    "    log_p2_0 = np.log(p2 + small_prob)\n",
    "    log_p2_1 = np.log(1 - p2 + small_prob)\n",
    "\n",
    "    log_pc1 = np.log(pc1 + small_prob)\n",
    "    log_pc2 = np.log(pc2 + small_prob)\n",
    "\n",
    "    # - Implement classification loop\n",
    "    for n in range(N):\n",
    "        x = X[n, :] \n",
    "        true_label = y[n]\n",
    "\n",
    "        # - For each sample, compute likelihood under each class\n",
    "        log_likelihood1 = np.sum(x * log_p1_1 + (1 - x) * log_p1_0)\n",
    "        log_likelihood2 = np.sum(x * log_p2_1 + (1 - x) * log_p2_0)\n",
    "  \n",
    "        log_posterior1 = log_likelihood1 + log_pc1\n",
    "        log_posterior2 = log_likelihood2 + log_pc2\n",
    "\n",
    "        # - Compare g_x = pc1 * likelihood1 - pc2 * likelihood2\n",
    "        if log_posterior1 > log_posterior2:\n",
    "            predicted_label = 1\n",
    "        else:\n",
    "            predicted_label = 2\n",
    "            \n",
    "        # - Count errors\n",
    "        if predicted_label != true_label:\n",
    "            errors += 1\n",
    "\n",
    "    test_error = errors / N\n",
    "    return test_error\n",
    "\n",
    "    # TODO: Implement classification loop\n",
    "    # - For each sample, compute likelihood under each class\n",
    "    # - Compare g_x = pc1 * likelihood1 - pc2 * likelihood2\n",
    "    # - Count errors\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4573d3e",
   "metadata": {
    "id": "c4573d3e"
   },
   "source": [
    "## Part C — `Bayes_Learning(X_tr, y_tr, X_val, y_val, priors)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d96b0539",
   "metadata": {
    "id": "d96b0539"
   },
   "outputs": [],
   "source": [
    "def Bayes_Learning(training_data, validation_data):\n",
    "    \"\"\"\n",
    "    Return:\n",
    "      p1, p2: Bernoulli parameters per class\n",
    "      pc1, pc2: best priors found on validation set\n",
    "    \"\"\"\n",
    "    # Define a list of candidate priors P(C1) to sweep (P(C2) = 1 - P(C1))\n",
    "    prior_candidates_pc1 = np.linspace(0.01, 0.99, 20) \n",
    "\n",
    "    # - Implement MLE on training data\n",
    "    p1_mle, p2_mle, _, _ = MLE_Learning(training_data)\n",
    "    \n",
    "    best_error = float('inf')\n",
    "    pc1_best = None\n",
    "    pc2_best = None\n",
    "    \n",
    "    results = [] \n",
    "\n",
    "    # - Sweep priors list\n",
    "    for pc1_can in prior_candidates_pc1:\n",
    "        pc2_can = 1.0 - pc1_can\n",
    "        \n",
    "        # - For each prior, compute validation error\n",
    "        current_error = Bayes_Testing(\n",
    "            data=validation_data, \n",
    "            p1=p1_mle, \n",
    "            p2=p2_mle, \n",
    "            pc1=pc1_can, \n",
    "            pc2=pc2_can\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            'P(C1) Prior': pc1_can, \n",
    "            'P(C2) Prior': pc2_can, \n",
    "            'Validation Error Rate': current_error\n",
    "        })\n",
    "        \n",
    "        # - Pick best prior\n",
    "        if current_error < best_error:\n",
    "            best_error = current_error\n",
    "            pc1_best = pc1_can\n",
    "            pc2_best = pc2_can\n",
    "            \n",
    "    if pc1_best is None:\n",
    "        pc1_best = 0.5\n",
    "        pc2_best = 0.5\n",
    "\n",
    "    validation_results_df = pd.DataFrame(results).round(4)\n",
    "    \n",
    "    return p1_mle, p2_mle, pc1_best, pc2_best, validation_results_df\n",
    "\n",
    "    # TODO: Implement MLE on training data\n",
    "    # TODO: Sweep priors list\n",
    "    # TODO: For each prior, compute validation error\n",
    "    # TODO: Pick best prior and return\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858c6031",
   "metadata": {
    "id": "858c6031"
   },
   "source": [
    "## Main HW1 Script\n",
    "\n",
    "**Part 1 (Toy sanity):**  \n",
    "- Load `toy_data.txt`, run `MLE_Learning`, and then compute training error using **pc1=pc2=0.5**.\n",
    "\n",
    "**Part 2 (Train/Test):**  \n",
    "- Fit on `training_data.txt` and test on `testing_data.txt` using **empirical priors** returned by `MLE_Learning`.\n",
    "\n",
    "**Part 3 (Validation sweep):**  \n",
    "- Fit on training, sweep priors on validation, choose the **best prior**, then test on `testing_data.txt`.\n",
    "- Print a small table for the validation errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "Hrve3jfR4rr1",
   "metadata": {
    "id": "Hrve3jfR4rr1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toy MLE parameters:\n",
      "p1: [0.8 1. ]\n",
      "p2: [0.2 0.6]\n",
      "pc1: 0.5 pc2: 0.5\n",
      "Training error (toy, pc1=pc2=0.5): 0.2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load datasets - paths mentioned above in setup portion\n",
    "training_data = np.loadtxt(TRAIN_PATH)\n",
    "validation_data = np.loadtxt(VALID_PATH)\n",
    "testing_data = np.loadtxt(TEST_PATH)\n",
    "\n",
    "# -------------------------\n",
    "# Part 1 — Toy sanity check\n",
    "# -------------------------\n",
    "toy_data = np.loadtxt(TOY_PATH)\n",
    "p1, p2, pc1, pc2 = MLE_Learning(toy_data)\n",
    "print(\"Toy MLE parameters:\")\n",
    "print(\"p1:\", p1)\n",
    "print(\"p2:\", p2)\n",
    "print(\"pc1:\", pc1, \"pc2:\", pc2)\n",
    "\n",
    "train_error = Bayes_Testing(toy_data, p1, p2, 0.5, 0.5)  # fixed priors for sanity check\n",
    "print(\"Training error (toy, pc1=pc2=0.5):\", train_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "QFK0FegK5dc_",
   "metadata": {
    "id": "QFK0FegK5dc_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error (empirical priors): 0.115\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Part 2 — Train/Test with empirical priors\n",
    "# -------------------------\n",
    "p1, p2, pc1, pc2 = MLE_Learning(training_data)\n",
    "test_error = Bayes_Testing(testing_data, p1, p2, pc1, pc2)\n",
    "print(\"Test error (empirical priors):\", test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "50a4edf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Validation Error Rates for Prior Sweep ---\n",
      "    P(C1) Prior  P(C2) Prior  Validation Error Rate\n",
      "0        0.0100       0.9900                  0.155\n",
      "1        0.0616       0.9384                  0.120\n",
      "2        0.1132       0.8868                  0.100\n",
      "3        0.1647       0.8353                  0.095\n",
      "4        0.2163       0.7837                  0.100\n",
      "5        0.2679       0.7321                  0.100\n",
      "6        0.3195       0.6805                  0.110\n",
      "7        0.3711       0.6289                  0.105\n",
      "8        0.4226       0.5774                  0.105\n",
      "9        0.4742       0.5258                  0.105\n",
      "10       0.5258       0.4742                  0.125\n",
      "11       0.5774       0.4226                  0.120\n",
      "12       0.6289       0.3711                  0.140\n",
      "13       0.6805       0.3195                  0.140\n",
      "14       0.7321       0.2679                  0.145\n",
      "15       0.7837       0.2163                  0.175\n",
      "16       0.8353       0.1647                  0.200\n",
      "17       0.8868       0.1132                  0.255\n",
      "18       0.9384       0.0616                  0.335\n",
      "19       0.9900       0.0100                  0.525\n",
      "\n",
      "Best Priors Found: P(C1) = 0.165, P(C2) = 0.835\n",
      "Minimum Validation Error: 0.0950\n",
      "\n",
      "--- Final Test Error Rate ---\n",
      "      Parameter  Value\n",
      "     Best P(C1)  0.165\n",
      "     Best P(C2)  0.835\n",
      "Test Error Rate 0.0850\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Part 3 — Validation sweep and best prior\n",
    "# -------------------------\n",
    "p1, p2, pc1_best, pc2_best, validation_df = Bayes_Learning(training_data, validation_data) \n",
    "test_error = Bayes_Testing(testing_data, p1, p2, pc1_best, pc2_best)\n",
    "\n",
    "# Display the table of validation errors\n",
    "print(\"\\n--- Validation Error Rates for Prior Sweep ---\")\n",
    "min_error_row = validation_df['Validation Error Rate'].idxmin()\n",
    "\n",
    "print(validation_df)\n",
    "print(f\"\\nBest Priors Found: P(C1) = {pc1_best:.3f}, P(C2) = {pc2_best:.3f}\")\n",
    "print(f\"Minimum Validation Error: {validation_df.loc[min_error_row, 'Validation Error Rate']:.4f}\")\n",
    "\n",
    "print(\"\\n--- Final Test Error Rate ---\")\n",
    "final_results_df = pd.DataFrame({\n",
    "    'Parameter': ['Best P(C1)', 'Best P(C2)', 'Test Error Rate'],\n",
    "    'Value': [f\"{pc1_best:.3f}\", f\"{pc2_best:.3f}\", f\"{test_error:.4f}\"]\n",
    "})\n",
    "print(final_results_df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1Wm_ecp2sAm6LVJSViQzaupU2dw2BzxOV",
     "timestamp": 1757026076706
    }
   ]
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
